{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"INM706, Seq2Seq.ipynb","provenance":[{"file_id":"1IBZ2LXVoVDAf5MsiAv2XiRjpzxl7hQkY","timestamp":1588241841894}],"collapsed_sections":["vjblRGwnJeF7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SynnFrpu_rRl","colab_type":"code","outputId":"d6cf05e9-d1f1-4a56-9480-025ce2b8cb5b","executionInfo":{"status":"ok","timestamp":1590320528694,"user_tz":-120,"elapsed":22924,"user":{"displayName":"CK XZ","photoUrl":"","userId":"14905850197231417667"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UZcGpIzJ2WaV","colab_type":"code","colab":{}},"source":["import re, os\n","import pickle\n","import itertools\n","import unicodedata\n","import codecs\n","import csv\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from torch.utils import data\n","import torch.nn.functional as F\n","#from torch.utils.tensorboard import SummaryWriter\n","#%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYFDnP7tCwNy","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/706')\n","import dataset_vf1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"943JZNPoJX1v","colab_type":"text"},"source":["# Load pre-processed data"]},{"cell_type":"code","metadata":{"id":"LNhFXcuqvi4w","colab_type":"code","colab":{}},"source":["# Paths\n","voc = pickle.load(open('/content/drive/My Drive/706/data_objects/voc.pkl', 'rb'))\n","pairs = pickle.load(open('/content/drive/My Drive/706/data_objects/pairs.pkl', 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vjblRGwnJeF7","colab_type":"text"},"source":["## If pickle error: preprocess data online"]},{"cell_type":"code","metadata":{"id":"Ea8Y1Je0Jfr6","colab_type":"code","colab":{}},"source":["corpus_name = 'Cornell Movie-Dialogs'\n","wd = os.getcwd()\n","datafile = os.path.join(wd, '706/data_objects/formatted_movie_lines.txt')\n","\n","# Special tokens\n","PAD_token = 0  # Enables padding all utterances to same length\n","SOS_token = 1  # Start-Of-Sentence token: added at the beginning of each utterance\n","EOS_token = 2  # End-Of-Sentence token: added at the end of each utterance\n","\n","\n","\n","class Voc:\n","\tdef __init__(self, name):\n","\t\tself.name = name\n","\t\tself.trimmed = False\n","\t\tself.word2index = {}\n","\t\tself.word2count = {}\n","\t\tself.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","\t\tself.num_words = 3  # SOS, EOS, PAD\n","\n","\tdef addSentence(self, sentence):\n","\t\tfor word in sentence.split(' '):\n","\t\t\tself.addWord(word)\n","\n","\tdef addWord(self, word):\n","\t\tif word not in self.word2index:\n","\t\t\tself.word2index[word] = self.num_words\n","\t\t\tself.word2count[word] = 1\n","\t\t\tself.index2word[self.num_words] = word\n","\t\t\tself.num_words += 1\n","\t\telse:\n","\t\t\tself.word2count[word] += 1\n","\n","\t# Remove irrelevant words (appearing less than arbitrary count threshold in data set)\n","\tdef trim(self, min_count):\n","\t\tif self.trimmed:\n","\t\t\treturn\n","\t\tself.trimmed = True\n","\n","\t\tkeep_words = []\n","\n","\t\tfor k, v in self.word2count.items():\n","\t\t\tif v >= min_count:\n","\t\t\t\tkeep_words.append(k)\n","\n","\t\tprint('keep_words {} / {} = {:.4f}'.format(\n","\t\t\tlen(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n","\t\t))\n","\n","\t\t# Reinitialize dictionaries\n","\t\tself.word2index = {}\n","\t\tself.word2count = {}\n","\t\tself.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","\t\tself.num_words = 3  # Count default tokens\n","\n","\t\tfor word in keep_words:\n","\t\t\tself.addWord(word)\n","\n","\n","# Max utterance length\n","MAX_LENGTH = 20\n","\n","\n","# encoding: from unicode to ASCII\n","def unicodeToAscii(s):\n","\treturn ''.join(\n","\t\tc for c in unicodedata.normalize('NFD', s)\n","\t\tif unicodedata.category(c) != 'Mn'\n","\t)\n","\n","\n","\n","def normalizeString(s):\n","\ts = unicodeToAscii(s.lower().strip())\n","\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n","\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","\ts = re.sub(r\"\\s+\", r\" \", s).strip()\n","\treturn s\n","\n","\n","# Returns constructed Voc class and a list of lists, each one containing a pair of utterances (question - answer like)\n","def readVocs(datafile, corpus_name):\n","\t# print(\"Reading lines...\")\n","\tlines = open(datafile, encoding='utf-8'). \\\n","\t\tread().strip().split('\\n')\n","\tpairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","\tvoc = Voc(corpus_name)\n","\treturn voc, pairs\n","\n","\n","# Returns True if both pair utterances are shorter than MAX_LENGTH\n","def filterPair(p):\n","\treturn len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","\n","\n","# EXPLICA!\n","def filterPairs(pairs):\n","\treturn [pair for pair in pairs if filterPair(pair)]\n","\n","\n","# Returns a populated Voc and list of filtered (all utterances shorter than MAX_LENGTH) pairs\n","def loadPrepareData(corpus_name, datafile):\n","\t# print(\"Start preparing training data ...\")\n","\tvoc, pairs = readVocs(datafile, corpus_name)\n","\t# print(\"Read {!s} sentence pairs\".format(len(pairs)))\n","\tpairs = filterPairs(pairs)\n","\t# print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n","\t# print(\"Counting words...\")\n","\tfor pair in pairs:\n","\t\tvoc.addSentence(pair[0])\n","\t\tvoc.addSentence(pair[1])\n","\t# print(\"Counted words:\", voc.num_words)\n","\treturn voc, pairs\n","\n","\n","voc, pairs = loadPrepareData(corpus_name, datafile)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9d0nAm33kZK","colab_type":"text"},"source":["# Modules: \n"]},{"cell_type":"markdown","metadata":{"id":"uRK7IYTE5xsi","colab_type":"text"},"source":["## GRU Encoder"]},{"cell_type":"code","metadata":{"id":"2e_W9fUtyBP1","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    def __init__(self, emb_dim, embedding, hidden_size, n_layers = 1, dropout = 0):\n","        super(Encoder, self).__init__()\n","        #self.emb_dim = emb_dim\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.wte = embedding\n","        self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first = False,\n","                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n","        #self.init_params()\n","\n","    def init_params(self):\n","      for x in self.named_parameters():\n","        x[1].requires_grad = True\n","        if 'weight' in x[0]:\n","          torch.nn.init.xavier_uniform_(x[1])\n","        elif 'bias' in x[0]:\n","          x[1].data.fill_(0.01)\n","\n","    def forward(self, input_seq, hidden = None):\n","        # embed input sequence\n","        embedded = self.wte(input_seq)\n","        # Forward-pass \n","        outputs, hidden = self.gru(embedded, hidden)\n","        # Sums bidirectional outputs from GRU\n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        return outputs, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hetVwZcAZEX","colab_type":"text"},"source":["## Luong's Attention"]},{"cell_type":"code","metadata":{"id":"hprRrt8TPxmf","colab_type":"code","colab":{}},"source":["class LuongAttention(nn.Module):\n","    def __init__(self, score_method, hidden_size, device = 'cpu'):\n","        super(LuongAttention, self).__init__()\n","        self.score_method = score_method\n","        if self.score_method not in ['dot', 'general', 'concat']:\n","            raise ValueError(self.score_method, 'is not an appropriate attention method')\n","        if self.score_method == 'general':\n","            self.attn = nn.Linear(hidden_size, hidden_size)\n","        elif self.score_method == 'concat':\n","            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n","            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n","        if torch.cuda.is_available():\n","            self.device = torch.device('cuda')\n","\n","    def dot_score(self, hidden, encoder_output):\n","        return torch.sum(hidden * encoder_output, dim = 2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim = 2)\n","\n","    def concat_score(self, hidden, encoder_output):\n","        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), 1, -1), encoder_output), 2)).tanh()\n","        return torch.sum(self.other * energy, dim = 2)\n","\n","    def forward(self, hidden, encoder_output):\n","        # Compute attention \"energies\" according to chosen mathematical model\n","        if self.score_method == 'dot':\n","            energies = self.dot_score(hidden, encoder_output)\n","        elif self.score_method == 'general':\n","            energies = self.general_score(hidden, encoder_output)\n","        elif self.score_method == 'concat':\n","            energies = self.concat_score(hidden, encoder_output)\n","        energies = energies.t()\n","        return F.softmax(energies, dim = 1).unsqueeze(1) # [batch_size, 1, max_length]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAwO3nySP0ze","colab_type":"text"},"source":["## \"Attentive\" GRU Decoder"]},{"cell_type":"code","metadata":{"id":"bSRZ66N33iSS","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    def __init__(self, vocab_size, emb_dim, embedding, hidden_size, attn_model, n_layers=1, dropout=0.1):\n","        super(Decoder, self).__init__()\n","\n","        #self.emb_dim = emb_dim\n","        #self.vocab_size = vocab_size\n","        self.n_layers = n_layers\n","        #self.dropout = dropout\n","        self.wte = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first = False,\n","                          dropout=(0 if n_layers == 1 else dropout))\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        self.attn = LuongAttention(attn_model, emb_dim)\n","        #self.init_params()\n","\n","    def init_params(self):\n","      for x in self.named_parameters():\n","        x[1].requires_grad = True\n","        if 'weight' in x[0]:\n","          torch.nn.init.xavier_uniform_(x[1])\n","        elif 'bias' in x[0]:\n","          x[1].data.fill_(0.01)\n","\n","    def forward(self, input_step, encoder_outputs, encoder_hidden):\n","        embedded = self.wte(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        # Forward-pas through GRU \n","        rnn_output, hidden = self.gru(embedded, encoder_hidden)\n","        # Compute attention's weights given the encoder output and \n","        attn_weights = self.attn(rnn_output, encoder_outputs)\n","        # Multipliquem pesos d'atenció amb el vector de context\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        # Concatenem nou vector de context (amb atenció) amb l'output de la xarxa GRU\n","        rnn_output = rnn_output.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((rnn_output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        # Predim resposta\n","        output = self.out(concat_output)\n","        output = F.softmax(output, dim=1)\n","  \n","        return output, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nHQaIge84Acw","colab_type":"text"},"source":["# Train Loop"]},{"cell_type":"markdown","metadata":{"id":"MKqKIY1EMdbS","colab_type":"text"},"source":["Loss"]},{"cell_type":"code","metadata":{"id":"GF2EOZ7RMcwI","colab_type":"code","colab":{}},"source":["def maskNLLLoss(inp, target, mask):\n","\tnTotal = mask.sum()\n","\t# Compute -log of probability assigned by decoder to correct next word\n","\tCrossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n","\tloss = CrossEntropy.masked_select(mask).mean()\n","\tloss = loss.to(device)\n","\treturn loss, nTotal.item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjd_eTfK75bh","colab_type":"text"},"source":["Define train paramaters RNN_1: hidden_size = 256, emb_dim = 30, encoder_n_layers = 2, decoder_n_layers = 2, lr = 0.0001"]},{"cell_type":"code","metadata":{"id":"zuM48G3874zm","colab_type":"code","outputId":"74c7b8da-bbbd-4941-e11d-d0cd3daabe01","executionInfo":{"status":"ok","timestamp":1590314992947,"user_tz":-120,"elapsed":5121,"user":{"displayName":"CK XZ","photoUrl":"","userId":"14905850197231417667"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["#Experiment name:\n","xp_name = 'RNN_1'\n","# Pretrained models at:\n","wd = '/content/drive/My Drive/706/checkpoints'\n","folder_name = os.path.join(wd, xp_name)\n","\n","# Get device right\n","device = 'cpu'\n","if (torch.cuda.is_available()):\n","    device = torch.device('cuda')\n","epochs = 100\n","batch_size = 64\n","\n","# DataLoader\n","ds = dataset_vf1.dataset(voc, pairs)\n","dataloader = data.DataLoader(ds, batch_size = batch_size, shuffle=False)\n","\n","# Encoder & Decoder params:\n","hidden_size = 256\n","emb_dim = 30\n","embedding = nn.Embedding(voc.num_words, emb_dim)\n","encoder_n_layers = 2\n","decoder_n_layers = 2\n","dropout = 0.1\n","lr = 0.0001\n","decoder_learning_ratio = 5.0\n","clip = 50\n","\n","# Teacher forcing?\n","teacher_forcing_ratio = 0.5\n","\n","# Attention method:\n","score_method = 'dot'\n","#score_method = 'general'\n","#score_method = 'concat'\n","\n","# Encoder:\n","encoder = Encoder(emb_dim = emb_dim, embedding = embedding, hidden_size = hidden_size, n_layers = encoder_n_layers, dropout = dropout)\n","encoder = encoder.to(device)\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr = lr)\n","encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', factor=0.5, patience=5)\n","\n","\n","#Decoder:\n","decoder = Decoder(vocab_size = voc.num_words, emb_dim = emb_dim, embedding = embedding, hidden_size = hidden_size, attn_model = score_method, n_layers = decoder_n_layers, dropout = dropout)\n","decoder = decoder.to(device)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr = lr * decoder_learning_ratio)\n","decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', factor=0.5, patience=5)\n","\n","\n","# Have a pre-trained model? If so set pretrained to True and load:\n","pretrained = True\n","if pretrained:\n","  try:\n","    checkpoint = torch.load(os.path.join(folder_name, 'checkpoint.pth'))\n","  except:\n","    raise FileNotFoundError(\"Unable to load checkpoint\")\n","  epoch = checkpoint['epoch'] + 1\n","\n","  encoder.load_state_dict(checkpoint['encoder'])\n","  encoder_optimizer.load_state_dict(checkpoint['encoder_optim'])\n","\n","  decoder.load_state_dict(checkpoint['decoder'])\n","  decoder_optimizer.load_state_dict(checkpoint['decoder_optim'])\n","\n","  embedding.load_state_dict(checkpoint['embedding'])\n","\n","encoder = encoder.to(device)\n","encoder.train()\n","decoder = decoder.to(device)\n","decoder.train()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Decoder(\n","  (wte): Embedding(33027, 30)\n","  (embedding_dropout): Dropout(p=0.1, inplace=False)\n","  (gru): GRU(30, 256, num_layers=2, dropout=0.1)\n","  (concat): Linear(in_features=512, out_features=256, bias=True)\n","  (out): Linear(in_features=256, out_features=33027, bias=True)\n","  (attn): LuongAttention()\n",")"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"uU1ggMJFF4h2","colab_type":"text"},"source":["Define train paramaters RNN_2: hidden_size = 512, emb_dim = 300, encoder_n_layers = 2, decoder_n_layers = 2, lr = 0.00001"]},{"cell_type":"code","metadata":{"id":"EE5M-RyAF3y2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"af6626a3-61cf-46cc-a7d9-d2333ff0be3a","executionInfo":{"status":"ok","timestamp":1590315645597,"user_tz":-120,"elapsed":15077,"user":{"displayName":"CK XZ","photoUrl":"","userId":"14905850197231417667"}}},"source":["#Experiment name:\n","xp_name = 'RNN_2'\n","# Pretrained models at:\n","wd = '/content/drive/My Drive/706/checkpoints'\n","folder_name = os.path.join(wd, xp_name)\n","\n","# Get device right\n","device = 'cpu'\n","if (torch.cuda.is_available()):\n","    device = torch.device('cuda')\n","epochs = 100\n","batch_size = 64\n","\n","# DataLoader\n","ds = dataset_vf1.dataset(voc, pairs)\n","dataloader = data.DataLoader(ds, batch_size = batch_size, shuffle=False)\n","\n","# Encoder & Decoder params:\n","hidden_size = 512\n","emb_dim = 300\n","embedding = nn.Embedding(voc.num_words, emb_dim)\n","encoder_n_layers = 2\n","decoder_n_layers = 2\n","dropout = 0.1\n","lr = 0.00001\n","decoder_learning_ratio = 5.0\n","clip = 50\n","\n","# Teacher forcing?\n","teacher_forcing_ratio = 0.5\n","\n","# Attention method:\n","score_method = 'dot'\n","#score_method = 'general'\n","#score_method = 'concat'\n","\n","# Encoder:\n","encoder = Encoder(emb_dim = emb_dim, embedding = embedding, hidden_size = hidden_size, n_layers = encoder_n_layers, dropout = dropout)\n","encoder = encoder.to(device)\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr = lr)\n","encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', factor=0.5, patience=5)\n","\n","\n","#Decoder:\n","decoder = Decoder(vocab_size = voc.num_words, emb_dim = emb_dim, embedding = embedding, hidden_size = hidden_size, attn_model = score_method, n_layers = decoder_n_layers, dropout = dropout)\n","decoder = decoder.to(device)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr = lr * decoder_learning_ratio)\n","decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', factor=0.5, patience=5)\n","\n","\n","# Have a pre-trained model? If so set pretrained to True and load:\n","pretrained = True\n","if pretrained:\n","  try:\n","    checkpoint = torch.load(os.path.join(folder_name, 'checkpoint.pth'))\n","  except:\n","    raise FileNotFoundError(\"Unable to load checkpoint\")\n","  epoch = checkpoint['epoch'] + 1\n","\n","  encoder.load_state_dict(checkpoint['encoder'])\n","  encoder_optimizer.load_state_dict(checkpoint['encoder_optim'])\n","\n","  decoder.load_state_dict(checkpoint['decoder'])\n","  decoder_optimizer.load_state_dict(checkpoint['decoder_optim'])\n","\n","  embedding.load_state_dict(checkpoint['embedding'])\n","\n","encoder = encoder.to(device)\n","encoder.train()\n","decoder = decoder.to(device)\n","decoder.train()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Decoder(\n","  (wte): Embedding(33027, 300)\n","  (embedding_dropout): Dropout(p=0.1, inplace=False)\n","  (gru): GRU(300, 512, num_layers=2, dropout=0.1)\n","  (concat): Linear(in_features=1024, out_features=512, bias=True)\n","  (out): Linear(in_features=512, out_features=33027, bias=True)\n","  (attn): LuongAttention()\n",")"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"4BEce0Z7MaLA","colab_type":"text"},"source":["Train loop"]},{"cell_type":"code","metadata":{"id":"GV5WmIRHMYcn","colab_type":"code","colab":{}},"source":["tb = SummaryWriter()\n","for epoch in range(epochs):\n","  print(f'Starting epoch {epoch}...')\n","  \n","  losses = []\n","  total_loss = 0\n","  n_totals = 0\n","\n","  for batch in dataloader:\n","    idx_, inputs, _, targets, masks, targets_len = batch\n","    bs = idx_.size()[0] \n","    # Get inquiries (inputs), replies (targets) and masks to proper shape for GRU: [seq_len, batch_size, emb_dim] (emb_dim is added by Encoder's embedder before passing seq through GRU)\n","    inputs, targets, masks = inputs.transpose(1, 0), targets.transpose(1, 0), masks.transpose(1, 0)\n","    # Get them to proper device\n","    inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # Initialize loss to zero\n","    batch_loss = 0\n","\n","    # Forward pass through encoder\n","    encoder_outputs, encoder_hidden = encoder(inputs)\n","\n","    # First input do decoder: SOS_token\n","    decoder_input = targets[0].unsqueeze(0)\n","    decoder_input = decoder_input.to(device)\n","\n","    decoder_hidden = encoder_hidden[:decoder.n_layers]\n","\n","    # Determine if teacher_forcing is used in this iteration\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    #  \n","    if use_teacher_forcing:\n","        for t in range(1, max(targets_len)):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, encoder_outputs, decoder_hidden\n","            )\n","\n","            decoder_input = targets[t].view(1, -1)\n","\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, targets[t], masks[t])\n","            batch_loss += mask_loss\n","            losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","    else:\n","        for t in range(1, max(targets_len)):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, encoder_outputs, decoder_hidden\n","            )\n","            _, topi = decoder_output.topk(1)\n","            decoder_input = torch.LongTensor([[topi[i][0] for i in range(bs)]])\n","            decoder_input = decoder_input.to(device)\n","\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, targets[t], masks[t])\n","            batch_loss += mask_loss\n","            losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","\n","    batch_loss.backward()\n","\n","    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n","    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    total_loss = (sum(losses) / n_totals)\n","    encoder_scheduler.step(total_loss)\n","    decoder_scheduler.step(total_loss)\n","\n","  tb.add_scalar('Total loss', total_loss, epoch)\n","\n","\n","tb.close()\n","\n","torch.save({'epoch': epoch, 'encoder': encoder.state_dict(), 'encoder_optim': encoder_optimizer.state_dict(),\n","            'decoder': decoder.state_dict() , 'decoder_optim': decoder_optimizer.state_dict(), 'embedding': embedding.state_dict()}, f'{folder_name}/checkpoint.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQUCBE8rPRDJ","colab_type":"text"},"source":["# Enable chatting"]},{"cell_type":"code","metadata":{"id":"1uVBkQbRPTfA","colab_type":"code","colab":{}},"source":["PAD_token = 0\n","SOS_token = 1\n","EOS_token = 2\n","\n","class GreedySearchDecoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(GreedySearchDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq):\n","        # Encode\n","        encoder_outputs, encoder_hidden = self.encoder(input_seq)\n","        # Prepare last encoder's hidden state for the decoder\n","        decoder_hidden = encoder_hidden[:decoder.n_layers]\n","        # Define first input to decoder\n","        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n","        # Create two tensors to add predicted tokens and scores respectively\n","        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n","        all_scores = torch.zeros([0], device=device)\n","        for _ in range(22):\n","            # Get token predicted with higher score by decoder \n","            decoder_output, decoder_hidden = self.decoder(decoder_input, encoder_outputs, decoder_hidden)\n","            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n","            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n","            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n","\n","            decoder_input = torch.unsqueeze(decoder_input, 0)\n","        return all_tokens, all_scores\n","\n","\n","def evaluate(encoder, decoder, searcher, voc, sentence):\n","    tokenized = list([SOS_token] + [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token])\n","    while len(tokenized) < 22: tokenized.append(PAD_token)\n","    tokenized = [tokenized]\n","    input_batch = torch.LongTensor(tokenized).transpose(0, 1)\n","    input_batch = input_batch.to(device)\n","    tokens, scores = searcher(input_batch)\n","    decoded_words = [voc.index2word[token.item()] for token in tokens]\n","    return decoded_words\n","\n","def unicodeToAscii(s):\n","\treturn ''.join(\n","\t\tc for c in unicodedata.normalize('NFD', s)\n","\t\tif unicodedata.category(c) != 'Mn'\n","\t)\n"," \n","def normalizeString(s):\n","\ts = unicodeToAscii(s.lower().strip())\n","\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n","\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","\ts = re.sub(r\"\\s+\", r\" \", s).strip()\n","\treturn s\n","\n","def evaluateInput(encoder, decoder, searcher, voc):\n","    input_sentence = ''\n","    while(1):\n","        try:\n","            # Ask for input sentence\n","            input_sentence = input('> ')\n","            # Is it to terminate conversation?\n","            if input_sentence == 'q' or input_sentence == 'quit': break\n","            # Normalize input sentence\n","            input_sentence = normalizeString(input_sentence)\n","            # Evaluate and... Build an answer!\n","            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n","            # Format and answer\n","            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n","            print('Bot:', ''.join(output_words))\n","\n","        except KeyError:\n","            print(\"Error: Encountered unknown word.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BS7cx8mfeLHv","colab_type":"text"},"source":["# Chat!"]},{"cell_type":"code","metadata":{"id":"Qnlc2ZlaPxhW","colab_type":"code","outputId":"cea6f3b3-a8a4-47cc-cdb3-c6979e0403e0","executionInfo":{"status":"ok","timestamp":1590315805242,"user_tz":-120,"elapsed":26958,"user":{"displayName":"CK XZ","photoUrl":"","userId":"14905850197231417667"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["encoder.eval()\n","decoder.eval()\n","\n","# Incialitzem mòdul \"search\"\n","searcher = GreedySearchDecoder(encoder, decoder)\n","\n","# Comença a xatejar\n","evaluateInput(encoder, decoder, searcher, voc)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["> hi\n","Bot: i...........\n","> What?\n","Bot: i............\n","> So..\n","Bot: i............\n","> Always the same?\n","Bot: i............\n","> 1\n","Bot: i...........\n","> q\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"au5EgTiPimQz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}